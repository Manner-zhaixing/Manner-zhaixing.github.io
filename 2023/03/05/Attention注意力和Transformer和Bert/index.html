<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>camille | camille</title><meta name="author" content="camille"><meta name="copyright" content="camille"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="transformer 通俗介绍通俗来讲：我们人脑每次看到东西摄入信息过多，容易造成信息过载，模型网络类似，这样就会使得网络效果变差，所以引入注意力机制，只关注我们感觉重要的部分，对想要的部分进行加大权值，其余部分赋予小的权值或者为0。 理论注意力机制用两种使用方式：1.放在其他经典网络中用来增强网络能力；2.单独作为模型使用  软注意力机制(soft attention mechanism)">
<meta property="og:type" content="article">
<meta property="og:title" content="camille">
<meta property="og:url" content="http://example.com/2023/03/05/Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8CTransformer%E5%92%8CBert/index.html">
<meta property="og:site_name" content="camille">
<meta property="og:description" content="transformer 通俗介绍通俗来讲：我们人脑每次看到东西摄入信息过多，容易造成信息过载，模型网络类似，这样就会使得网络效果变差，所以引入注意力机制，只关注我们感觉重要的部分，对想要的部分进行加大权值，其余部分赋予小的权值或者为0。 理论注意力机制用两种使用方式：1.放在其他经典网络中用来增强网络能力；2.单独作为模型使用  软注意力机制(soft attention mechanism)">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-03-05T13:10:48.134Z">
<meta property="article:modified_time" content="2023-03-05T13:10:26.027Z">
<meta property="article:author" content="camille">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/03/05/Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8CTransformer%E5%92%8CBert/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'camille',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-05 21:10:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="camille"><span class="site-name">camille</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">No title</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-03-05T13:10:48.134Z" title="Created 2023-03-05 21:10:48">2023-03-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-03-05T13:10:26.027Z" title="Updated 2023-03-05 21:10:26">2023-03-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a name="mh7gM"></a></p>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">transformer</a></h2><p><a name="D5oTz"></a></p>
<h2 id="通俗介绍"><a href="#通俗介绍" class="headerlink" title="通俗介绍"></a>通俗介绍</h2><p>通俗来讲：我们人脑每次看到东西摄入信息过多，容易造成信息过载，模型网络类似，这样就会使得网络效果变差，所以引入注意力机制，只关注我们感觉重要的部分，对想要的部分进行加大权值，其余部分赋予小的权值或者为0。<br><a name="j5Qqr"></a></p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p><strong>注意力机制用两种使用方式：1.放在其他经典网络中用来增强网络能力；2.单独作为模型使用</strong></p>
<ul>
<li><strong>软注意力机制(soft attention mechanism)</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665322122843-38215909-47ca-4b7f-be12-54152218b213.png#averageHue=%23f9f8f7&clientId=ucb672ac0-9136-4&from=ui&height=298&id=uf002dc9f&name=%E8%BD%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.png&originHeight=370&originWidth=341&originalType=binary&ratio=1&rotation=0&showTitle=false&size=51861&status=done&style=none&taskId=u457e5dc1-53f5-47c9-9e65-30d9739bbaf&title=&width=275" alt="软注意力机制.png"><br />上图中，q是查询向量，x为输入向量。步骤为q和x计算相似度，经过打分函数s，得到数据之后进行softmax运算，得到a，之后进行加权平均<br />分为两步：(1)计算注意力分布a，<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665322191889-34aafda1-10e4-4ad2-aa81-ab29ddf0086c.png#averageHue=%23f5f4f4&clientId=ucb672ac0-9136-4&from=ui&height=185&id=u88223d54&name=%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E5%B8%83.png&originHeight=215&originWidth=437&originalType=binary&ratio=1&rotation=0&showTitle=false&size=37955&status=done&style=none&taskId=u5ee8168c-9965-4b87-ad1f-7f32cde4507&title=&width=377" alt="注意力分布.png"><br />(2)根据a来计算输入信息的加权平均：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665322239882-60340dad-6640-429c-9fc6-7334f9cb6662.png#averageHue=%23f6f6f6&clientId=ucb672ac0-9136-4&from=ui&height=126&id=u8c002299&name=%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.png&originHeight=140&originWidth=287&originalType=binary&ratio=1&rotation=0&showTitle=false&size=12127&status=done&style=none&taskId=u8d0f776a-0435-407d-a704-ed9c4b5638b&title=&width=258" alt="加权平均.png"><br />其中s打分函数有以下几种：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665322881009-8c05263e-5630-4ba6-bdf9-198f6998aa24.png#averageHue=%23f5f5f5&clientId=ucb672ac0-9136-4&from=ui&height=207&id=u4c082502&name=%E6%89%93%E5%88%86%E5%87%BD%E6%95%B0.png&originHeight=340&originWidth=711&originalType=binary&ratio=1&rotation=0&showTitle=false&size=58176&status=done&style=none&taskId=u1e0e23a5-6f98-4281-96ce-915a23064cf&title=&width=433" alt="打分函数.png"><br />上述四个打分函数效果接近，所以近阶段选择更多的是能更好利用GPU性能的函数，点积模型用的较多</p>
<p><a name="cipD1"></a></p>
<h2 id="注意力机制的变体"><a href="#注意力机制的变体" class="headerlink" title="注意力机制的变体"></a>注意力机制的变体</h2><ul>
<li><strong>硬性注意力(hard attention)</strong></li>
</ul>
<p>在概率设置的时候，只有0或者1(区别于软注意力机制的0-1之间的值)</p>
<ul>
<li><strong>键值对注意力(key-value pair attention)</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665323389645-f28d40da-1268-4750-8794-3b8c776610cb.png#averageHue=%23fbfafa&clientId=ucb672ac0-9136-4&from=ui&height=298&id=u92e53bdc&name=%E9%94%AE%E5%80%BC%E5%AF%B9%E6%B3%A8%E6%84%8F%E5%8A%9B.png&originHeight=478&originWidth=1163&originalType=binary&ratio=1&rotation=0&showTitle=false&size=133121&status=done&style=none&taskId=udc9c1cbc-8f1b-4846-bb31-aad83dee295&title=&width=725" alt="键值对注意力.png"><br />在第二步中，计算出a之后，还要使用v的值进行加权</p>
<ul>
<li><strong>多头注意力(multi-head attention)</strong></li>
</ul>
<p>利用多个查询Q&#x3D;[q1,q2,,,,qm]，同时从输入信息中选取多组信息，每个注意力头关注输入信息的不同部分。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665323541427-41867191-28ad-4616-8080-2567e4e36d2f.png#averageHue=%23f0f0f0&clientId=ucb672ac0-9136-4&from=ui&height=64&id=u1853efe0&name=%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B.png&originHeight=98&originWidth=742&originalType=binary&ratio=1&rotation=0&showTitle=false&size=27904&status=done&style=none&taskId=u94cc5da1-fb79-4b4a-a83e-3e3cfed5aba&title=&width=486" alt="多头注意力.png"></p>
<ul>
<li><strong>结构化注意力(用的较少)</strong></li>
<li><strong>指针网络(pointer netword)(通常用在其他的神经网络中)</strong></li>
</ul>
<p>只利用注意力机制的第一步，将注意力分布作为一个软性的指针来指出相关信息的位置。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665324010482-c7ef6037-af4b-412e-ae35-7454ea9d43d2.png#averageHue=%23f9f7f5&clientId=ucb672ac0-9136-4&from=ui&height=243&id=ud868fe47&name=%E6%8C%87%E9%92%88%E7%BD%91%E7%BB%9C.png&originHeight=408&originWidth=931&originalType=binary&ratio=1&rotation=0&showTitle=false&size=93866&status=done&style=none&taskId=u081fe259-d6a6-48d4-96ad-fb853abf4ac&title=&width=555" alt="指针网络.png"><br />在找到想要东西的时候不需要存储向量，只要指针指出指回来即可</p>
<p><a name="O5nhQ"></a></p>
<h2 id="自注意力模型"><a href="#自注意力模型" class="headerlink" title="自注意力模型"></a>自注意力模型</h2><ul>
<li><strong>自注意力的图解表示</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665366883474-43b1e0b5-abdf-40ba-aa79-7459e474788a.png#averageHue=%23f8f8f7&clientId=u31b2fb40-6cd7-4&from=ui&height=299&id=ua4b1bf1e&name=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.png&originHeight=585&originWidth=1031&originalType=binary&ratio=1&rotation=0&showTitle=false&size=287259&status=done&style=none&taskId=ubed5ac85-30fe-47d1-afe8-a18db08fd18&title=&width=527" alt="自注意力机制.png"></p>
<p>the对每个词都进行相关性计算，然后进入到softmax函数，得到w概率，之后进行加权计算，得到the的上下文表示的向量。(softmax右侧的the的向量为q查询向量，所以说在自注意力模型中，查询向量q由句子本身自己提供，而不是从外部获取，所以叫自注意力模型)</p>
<ul>
<li><strong>自注意力模型的矩阵表示</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665367684529-68856192-551b-43d3-8e65-0f2ea7306a39.png#averageHue=%23fcf2f1&clientId=u31b2fb40-6cd7-4&from=ui&height=295&id=u8d1da7a2&name=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.png&originHeight=744&originWidth=1604&originalType=binary&ratio=1&rotation=0&showTitle=false&size=604538&status=done&style=none&taskId=u3be8d1f5-c0ba-4f2a-978b-bf37ddb6a8f&title=&width=637" alt="自注意力的矩阵表示.png"><br />左侧部分：词向量与w权重矩阵进行乘积，得到dx<br />右侧部分：w矩阵的动态计算方式：，</p>
<ul>
<li><strong>QKV模式(Query-Key-Value)-自注意力模型</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665368767278-47bbe463-ef84-452a-be0d-4f1a932951f0.png#averageHue=%23faf9f7&clientId=u31b2fb40-6cd7-4&from=ui&height=311&id=u79618249&name=QKV%E6%A8%A1%E5%BC%8F.png&originHeight=611&originWidth=1144&originalType=binary&ratio=1&rotation=0&showTitle=false&size=169822&status=done&style=none&taskId=uc37c570c-8ef0-4cb1-9460-35803aaad7f&title=&width=583" alt="QKV模式.png"><br />在上述的普通的自注意力模型中，计算权重矩阵和最后的点积运算公式固定，所以不可以进行参数学习，在QKV模式中，wq和wk和wv三个矩阵都是可以学习的</p>
<p>下图是计算机制，输入为x1,x2，想转换为z1，z2<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665486901089-6ec85d5b-caa9-4f6b-826e-f64c0545abe6.png#averageHue=%23f9f9f8&clientId=u49334b54-10db-4&from=ui&height=367&id=u21320f6f&name=self-attention1.png&originHeight=440&originWidth=857&originalType=binary&ratio=1&rotation=0&showTitle=false&size=146024&status=done&style=none&taskId=ud4a9345b-4b77-4ded-a7b4-c1f77716ade&title=&width=715" alt="self-attention1.png"><br />先将x1和x2通过三个Wq，Wk,Wv三个矩阵转换为q1,k1,v1和q2,k2,v2，即：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665487006627-30a954da-aeac-4fc8-bbc1-c64053c6419f.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&height=127&id=ue2d1d004&name=qkv.png&originHeight=138&originWidth=278&originalType=binary&ratio=1&rotation=0&showTitle=false&size=10170&status=done&style=none&taskId=uafa21ad0-0aa2-48d7-9753-b302b7e04c5&title=&width=255" alt="qkv.png"><br />在上述过程中，不同的输入x共享了同一套矩阵w，已经发生了一定程度上的信息互换。接下来计算z1，z2：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665487087975-758dc134-440b-42d6-95a3-317532aa46d8.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&height=74&id=ub3a41e2b&name=z1z2.png&originHeight=81&originWidth=221&originalType=binary&ratio=1&rotation=0&showTitle=false&size=3539&status=done&style=none&taskId=uf0c2b4f9-e2d0-424c-8593-75c9709a5ab&title=&width=202" alt="z1z2.png"><br />那么权重θ通过下面公式得到：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665487207564-1f87a417-db32-4f9d-a4c1-23eadfd02c86.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&height=158&id=ua9c78b2f&name=%E7%BA%BF%E6%80%A7%E5%8A%A0%E6%9D%83.png&originHeight=169&originWidth=385&originalType=binary&ratio=1&rotation=0&showTitle=false&size=16952&status=done&style=none&taskId=uea933e37-10de-44f4-9dbf-e325617dba6&title=&width=359" alt="线性加权.png"><br />最终公式为：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665487263096-178e515e-e9a2-4e8e-b36a-a440bb6ae0f0.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&height=108&id=u97b6aa25&name=%E6%9C%80%E7%BB%88%E5%85%AC%E5%BC%8F.png&originHeight=122&originWidth=457&originalType=binary&ratio=1&rotation=0&showTitle=false&size=9834&status=done&style=none&taskId=ue458cc20-9b1e-4f68-9883-ee95a7dd7d4&title=&width=405" alt="最终公式.png"><br />以上运算均为矩阵运算，这样能够充分利用GPU的并行计算能力，</p>
<ul>
<li><strong>多头自注意力机制</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665370536351-ee58e57a-45bb-4e2d-b2d1-99894a1a70ea.png#averageHue=%23fbf9f7&clientId=u31b2fb40-6cd7-4&from=ui&height=341&id=uc4f33171&name=%E5%A4%9A%E5%A4%B4QKV.png&originHeight=637&originWidth=1077&originalType=binary&ratio=1&rotation=0&showTitle=false&size=147788&status=done&style=none&taskId=udf042864-c85d-497c-9880-3533182e47d&title=&width=577" alt="多头QKV.png"><br />像之前的QKV结构多来几次，最后拼接到一起，使得模型更加灵活。<br />注意此时每一部分的w三个矩阵都是不一样的，产生多个结果z后，最后拼接到一起</p>
<p><a name="cHENj"></a></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">transformer详解</a><br /><a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer">self-attention和transformer</a></p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665398757501-3241527a-c38e-4dce-8a07-f46f404969d7.png#averageHue=%23e1e2c4&clientId=uc1c5cd53-2a6e-4&from=ui&height=406&id=u773287de&name=transformer.png&originHeight=674&originWidth=733&originalType=binary&ratio=1&rotation=0&showTitle=false&size=212595&status=done&style=none&taskId=u08c4d0af-297d-4821-8b78-7c90aa3f9d9&title=&width=442" alt="transformer.png"></p>
<p>相当于注意力机制的单独使用(不和CNN,RNN等联合使用)</p>
<ul>
<li><strong>形象表示：</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665399428780-5a9e10e2-5ed5-4448-84fc-6a114ec86311.png#averageHue=%23f6f6f6&clientId=uc1c5cd53-2a6e-4&from=ui&height=339&id=u9fc91496&name=tran.png&originHeight=691&originWidth=1146&originalType=binary&ratio=1&rotation=0&showTitle=false&size=317799&status=done&style=none&taskId=u1b698779-94de-4068-86bc-affa757a1b0&title=&width=563" alt="tran.png"><br />这种方式下，挨个节点进行与其他节点计算上下文信息的操作，而如果是序列化的模型，则需要一步一步进行操作计算，</p>
<ul>
<li><p>缺点：没法处理序列太长的文档，复杂度较高</p>
</li>
<li><p>使用transformer的原因：</p>
</li>
</ul>
<p>transformer机制中抛弃了传统的CNN和RNN，整个模型结构完全由Attention机制组成，作者采用Attention的原因是因为RNN的计算限制是顺序的，也就是说RNN相关算法只能从左到右依次计算或者从右向左依次计算，这种机制带来了两个问题：<br />1.时间t时刻的计算依赖于时间t-1时刻的计算结果，导致模型无法有效的并行计算<br />2.顺序计算的过程会导致数据信息的丢失，即，长程依赖问题。<br />针对着两个问题，Transformer的提出解决了这两个问题：<br />1.使用attention，将序列中任意两个位置之间的距离缩小为一个常量<br />2.不采用顺序结构，因此有很好的并行性，更好的利用GPU</p>
<ul>
<li>模型结构</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665478829023-6e6f366a-eea1-4448-a8ab-303157d2037c.png#averageHue=%23c0b598&clientId=u49334b54-10db-4&from=ui&height=539&id=ud393caeb&name=transformer%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png&originHeight=744&originWidth=613&originalType=binary&ratio=1&rotation=0&showTitle=false&size=200166&status=done&style=none&taskId=u244f4a40-1e3f-406b-a2f6-55c28e27b82&title=&width=444" alt="transformer模型结构.png"><br />整个结构分为两个部分：<br /><strong>Encoder：</strong>输入的是单词的Embedding(词向量),再加上位置编码，之后进入一个统一的结构(这个结构可以循环很多次)，也就是说有很多层，每一层可以分为Attention层和全连接层，在额外加了一些操作，例如skip connection，做条约链接，之后加入了normalization层。<br /><strong>Decoder</strong>：第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个模块(同样可以重复很多次),该模块分为三部分，第一部分为Attention，第二部分为cross Attention，第三部分为全连接层，也使用了跳跃连接和normalization<br />输出：最后的输出要通过Linear层(全连接层),在通过softmax做预测。</p>
<p><a name="FUUOJ"></a></p>
<h3 id="Encoding部分："><a href="#Encoding部分：" class="headerlink" title="Encoding部分："></a>Encoding部分：</h3><p>简化一下，只展示encoding和decoding中的一个模块</p>
<ul>
<li>输入部分：word embeding</li>
<li>self-attention：上述内容已经讲解</li>
<li>位置编码：输入的时候，不仅需要词向量，还需要位置编码，position embedding，即实际上的输入是词向量与位置编码的加和</li>
</ul>
<p>这里加入位置编码的作用：因为Transformer模型没有使用RNN等的序列方式，所以想要利用词的顺序，只能加入位置编码，维度与输入的词向量相同，所以可以直接相加</p>
<ul>
<li>skip connection和Layer Normalization</li>
</ul>
<p>Add &amp; Norm模块接在Encoder端和Decoder端每个子模块的后面，其中Add表示残差连接，Norm表示LayerNorm。<br />因此Encoder端和Decoder端每个子模块实际的输出为：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665488083811-bb6d42a0-fbf7-4c5a-8de0-655b475620a1.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&height=61&id=u1508033f&name=la%E5%92%8Cnorm%E7%9A%84%E5%AE%9E%E9%99%85%E8%BE%93%E5%87%BA.png&originHeight=69&originWidth=379&originalType=binary&ratio=1&rotation=0&showTitle=false&size=7771&status=done&style=none&taskId=u53f172c7-0a08-4c9d-8ae6-28e3680d167&title=&width=333" alt="la和norm的实际输出.png"><br />其中sublayer为子模块的输出。<br />skip-connect在模型中也有所体现：如下图虚线<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665488388375-f0debaa2-689f-42a9-98ff-b76f5f309508.png#averageHue=%23d9e8ca&clientId=u49334b54-10db-4&from=ui&height=289&id=ua14ffa80&name=skip.png&originHeight=613&originWidth=909&originalType=binary&ratio=1&rotation=0&showTitle=false&size=243556&status=done&style=none&taskId=uce48337b-9448-4938-ae4e-c680f908b3c&title=&width=429" alt="skip.png"><br />同时，还用了Normalize，用的是一种新的Layer Normalize，不是常用的Batch Normalize。是一种正则化的策略，避免网络过拟合。<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665489893595-5b2cdad7-54a3-48fd-be14-8a16ca8f9d9f.png#averageHue=%23fefefe&clientId=u49334b54-10db-4&from=ui&id=uc8b59104&name=layer%20nor%EF%BC%8C.png&originHeight=309&originWidth=1155&originalType=binary&ratio=1&rotation=0&showTitle=false&size=48048&status=done&style=none&taskId=u714d6f31-ed96-4e54-8753-9645a8c2aed&title=" alt="layer nor，.png"></p>
<ul>
<li><strong>encoding汇总</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665489927022-9ca7caca-c491-4ed1-90db-50f120fc1926.png#averageHue=%23e3edcf&clientId=u49334b54-10db-4&from=ui&height=408&id=ub4cc2782&name=encoding%E6%B1%87%E6%80%BB.png&originHeight=679&originWidth=687&originalType=binary&ratio=1&rotation=0&showTitle=false&size=234685&status=done&style=none&taskId=u434f3740-96d0-43d8-9e26-7ed7aa5797d&title=&width=413" alt="encoding汇总.png"></p>
<p><a name="VK48R"></a></p>
<h3 id="decoder部分："><a href="#decoder部分：" class="headerlink" title="decoder部分："></a>decoder部分：</h3><p>Decoder也是N&#x3D;6层堆叠的结构。被分为3个SubLayer，Encoder与Decoder有三大主要的不同：<br />1.Decoder SubLayer-1使用的是“<strong>Masked</strong>” Multi-Headed Attention机制，防止为了模型看到要预测的数据，防止泄露。<br /> 2.SubLayer-2是一个Encoder-Decoder Multi-head Attention。<br />3.LinearLayer和SoftmaxLayer作用于SubLayer-3的输出后面，来预测对应的word的probabilities 。</p>
<ul>
<li><strong>Decoder的Mask-Multi-Head-Attention输入端</strong></li>
</ul>
<p>模型训练阶段：<br />1.Decoder的初始输入：训练集的标签Y，并且需要整体右移(Shifted Right)一位<br />2.Shifted Right的原因：T-1时刻需要预测T时刻的输出，所以Decoder的输入需要整体右移一位<br />举例：我爱中国—&gt;I Love China<br />位置关系：<br />0-“I”<br />1-“Love”<br />2-“China”<br />操作：整体右移一位（Shifted Right）<br />0-</s>【起始符】目的是为了预测下一个Token<br />1-“I”<br />2-“Love”<br />3-“China”<br />具体步骤：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665536929857-4dfbf554-65dd-4cc1-b5d9-34494e4e2ce8.png#averageHue=%23fefefe&clientId=u1eb01406-55c6-4&from=ui&height=397&id=uf46dfa30&name=%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4.png&originHeight=644&originWidth=684&originalType=binary&ratio=1&rotation=0&showTitle=false&size=83067&status=done&style=none&taskId=u816e18f8-266c-460e-8f22-24ef0423421&title=&width=422" alt="具体步骤.png"><br /><strong>Transformer Decoder的输入：</strong></p>
<ul>
<li>初始输入：前一时刻Decoder输入+前一时刻Decoder的预测结果 + Positional Encoding(下图中标记1)</li>
<li>中间输入：Encoder Embedding(下图中标记2)</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665537048747-f04dd5f6-379a-4d79-807a-e675c511dade.png#averageHue=%23c3b799&clientId=u1eb01406-55c6-4&from=ui&height=407&id=u7dadcfc3&name=%E6%A0%87%E8%AE%B0%E8%BE%93%E5%85%A5.png&originHeight=666&originWidth=573&originalType=binary&ratio=1&rotation=0&showTitle=false&size=218761&status=done&style=none&taskId=ua5a6e68b-d7cb-4a64-b307-10fa89f30a1&title=&width=350" alt="标记输入.png"></p>
<p><strong>Mask：</strong><br />mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer模型里面涉及两种mask，分别是 padding mask和sequence mask。 其中，padding mask在所有的scaled dot-product attention 里面都需要用到，而sequence mask只有在Decoder的Self-Attention里面用到。<br /><strong>Padding Mask</strong><br />因为每个批次输入序列长度是不一样的我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置没有实际意义，所以我们的Attention机制不应该把注意力放在这些位置上。<br />具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！，而padding mask 实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。<br /><strong>Sequence mask</strong><br />文章前面也提到，sequence mask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。<br />sequence mask的目的是防止Decoder “seeing the future”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665537902073-f35e905a-c271-4824-90b7-433b564086d4.png#averageHue=%23eaeff3&clientId=u1eb01406-55c6-4&from=ui&height=223&id=u2acb3343&name=mask_%E7%9F%A9%E9%98%B5.png&originHeight=319&originWidth=398&originalType=binary&ratio=1&rotation=0&showTitle=false&size=14897&status=done&style=none&taskId=u9c3c9f05-6eaf-4bfc-9f5c-f9a33c46ad0&title=&width=278" alt="mask_矩阵.png"><br />对于Decoder的Self-Attention，里面使用到的scaled dot-product attention，同时需要padding mask和sequence mask作为attn_mask，具体实现就是两个mask相加作为attn_mask。<br />其他情况，attn_mask一律等于padding mask。</p>
<ul>
<li><strong>Decoder的Encode-Decode注意力层</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665539191686-aecd3c9c-8e39-45cc-8c36-ac68c4212812.png#averageHue=%23fefefe&clientId=u1eb01406-55c6-4&from=ui&height=213&id=u5fea0feb&name=decoder%E5%B1%82.png&originHeight=307&originWidth=1151&originalType=binary&ratio=1&rotation=0&showTitle=false&size=66651&status=done&style=none&taskId=u3ac4c7cc-f8e1-49ec-8d8f-8dc72302f0f&title=&width=800.4000244140625" alt="decoder层.png"></p>
<ul>
<li>Decoder的输出</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665540412690-f6c9c201-c789-4455-9f3c-234977c02a73.png#averageHue=%23d6e3cf&clientId=u1eb01406-55c6-4&from=ui&height=376&id=u63862401&name=decoder%E8%BE%93%E5%87%BA.png&originHeight=583&originWidth=1035&originalType=binary&ratio=1&rotation=0&showTitle=false&size=413812&status=done&style=none&taskId=u89d79170-9a99-4479-aec7-ca8951872ef&title=&width=668.4000244140625" alt="decoder输出.png"><br />从上图可以看出，Decoder和Encoder唯一的区别就是多了一个Encode-Decode注意力层，然后最后一层接了个linear+softmax层，损失函数就是交叉熵损失。<br />Decoder的最后一个部分是过一个linear layer将decoder的输出扩展到与vocabulary size一样的维度上。经过softmax 后，选择概率最高的一个word作为预测结果。假设我们有一个已经训练好的网络，<strong>在做预测时，步骤如下：</strong></p>
<ul>
<li>1.给Decoder输入Encoder对整个句子embedding的结果和一个特殊的开始符号</s>。Decoder 将产生预测，在我们的例子中应该是 ”I”。</li>
<li>2.给Decoder输入Encoder的embedding结果和</s> I，在这一步Decoder应该产生预测 am。</li>
<li>3.给Decoder输入Encoder的embedding结果和</s> I am，在这一步Decoder应该产生预测a。</li>
<li>4.给Decoder输入Encoder的embedding结果和</s> I am a，在这一步Decoder应该产生预测student。</li>
<li>5.给Decoder输入Encoder的embedding结果和</s> I am a student, Decoder应该生成句子结尾的标记，Decoder 应该输出</eos>。</li>
<li>6.然后Decoder生成了</eos>，翻译完成。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">代码</a></p>
<p><a name="wvZ2e"></a></p>
<h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">原论文</a><br /><strong>BERT</strong>的全称为Bidirectional Encoder Representation from Transformers。<br />主体结构：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665905096439-2947cef7-f331-4a1c-9e73-7bf834469cc7.png#averageHue=%23d6c78a&clientId=u3557d04a-1f16-4&from=ui&height=427&id=ub9ab63f2&name=bert.png&originHeight=476&originWidth=501&originalType=binary&ratio=1&rotation=0&showTitle=false&size=55378&status=done&style=none&taskId=u7b8fa911-2250-44a3-8f91-f23b41692b9&title=&width=449" alt="bert.png"><br />其中该部分是transformer叠加的结果：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665905152038-dbe1c644-7612-4e1f-8551-0e41f31beee1.png#averageHue=%23c0d1ee&clientId=u3557d04a-1f16-4&from=ui&height=206&id=ufceed5cf&name=%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86.png&originHeight=190&originWidth=427&originalType=binary&ratio=1&rotation=0&showTitle=false&size=29666&status=done&style=none&taskId=u8276e257-e5ac-41f6-bead-1c390e97fe6&title=&width=462" alt="输入部分.png"><br><a name="mpV9u"></a></p>
<h3 id="1-输入部分"><a href="#1-输入部分" class="headerlink" title="1.输入部分"></a>1.输入部分</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665905191154-55590ad4-4b03-4bce-b14c-d2e7cc3321a4.png#averageHue=%23e3dbcf&clientId=u3557d04a-1f16-4&from=ui&height=245&id=u8900237e&name=%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%861.png&originHeight=217&originWidth=427&originalType=binary&ratio=1&rotation=0&showTitle=false&size=21628&status=done&style=none&taskId=ucdc79488-1f78-4e94-86e0-f07d909c07a&title=&width=482" alt="输入部分1.png"></p>
<p>BERT的输入为每一个token对应的表征（图中的粉红色块就是token，黄色块就是token对应的表征），并且单词字典是采用<strong>WordPiece</strong>算法来进行构建的。为了完成具体的分类任务，除了单词的token之外，作者还在输入的每一个序列开头都插入特定的<strong>分类token（[CLS]）</strong>，该分类token对应的最后一个Transformer层输出被用来起到聚集整个序列表征信息的作用</p>
<ul>
<li>输入部分的组成</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665905847891-be7cce16-5d75-4cea-9fbb-1154ea4811a6.png#averageHue=%23ede7e3&clientId=u3557d04a-1f16-4&from=ui&height=238&id=uf492f54b&name=%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90.png&originHeight=302&originWidth=969&originalType=binary&ratio=1&rotation=0&showTitle=false&size=44443&status=done&style=none&taskId=u33672742-c5db-4417-887a-7e0082af8ef&title=&width=763.4000244140625" alt="输入部分组成.png"><br />针对上述三个部分的说明解释：<br /><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665905877438-8415e761-0df8-481f-a008-05a4f098f185.png#averageHue=%23f9f9f9&clientId=u3557d04a-1f16-4&from=ui&height=386&id=ubf8fa20f&name=%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E8%A7%A3%E9%87%8A.png&originHeight=444&originWidth=855&originalType=binary&ratio=1&rotation=0&showTitle=false&size=127782&status=done&style=none&taskId=u09f496fb-822d-4719-8a02-93289ac755c&title=&width=743.4000244140625" alt="输入部分解释.png"></p>
<p><a name="etkfg"></a></p>
<h3 id="2-输出部分"><a href="#2-输出部分" class="headerlink" title="2.输出部分"></a>2.输出部分</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/29355861/1665906018476-36b59197-bce8-416a-a3a9-39aec3f2c53e.png#averageHue=%23cedac1&clientId=u3557d04a-1f16-4&from=ui&height=119&id=u9d9cc875&name=%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86.png&originHeight=127&originWidth=490&originalType=binary&ratio=1&rotation=0&showTitle=false&size=12631&status=done&style=none&taskId=u6ba2590e-96b6-4311-a1d7-8e8c9722f8c&title=&width=460" alt="输出部分.png"><br /><strong>C</strong>为分类token（[CLS]）对应最后一个Transformer的输出，Ti 则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务_（如，序列标注和问答任务）_，就把Ti 输入到额外的输出层中进行预测。对于一些句子级别的任务_（如，自然语言推断和情感分类任务）_，就把<strong>C</strong>输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。</p>
<p><a name="aQGpT"></a></p>
<h3 id="3-Masked-Language-Model-MLM-第一个预训练任务"><a href="#3-Masked-Language-Model-MLM-第一个预训练任务" class="headerlink" title="3.Masked Language Model(MLM)(第一个预训练任务)"></a>3.Masked Language Model(MLM)(第一个预训练任务)</h3><p>MLM：随机屏蔽掉部分输入token，然后再去预测这些被屏蔽掉的token。<br />MLM是bert能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用mask token随机地对每一个训练序列中的token进行替换，然后利用该token的上下文预测出mask原有位置的单词。<br />但是采取以上策略会导致这样一个问题：mask不会出现在下游任务的微调阶段(fine-tuning)阶段，因此预训练阶段和微调阶段产生了不匹配（就是预训练的目标会令产生的语言表征对mask敏感，但是却对其他token不敏感），因此bert采取下列的策略来解决这个问题：<br />首先在每一个训练序列中以15%的概率随机地选中某个token位置用于预测，加入是第i个token被选中，则会被替换成以下三个token之一：</p>
<ul>
<li>80%的时候是mask，如： my dog is hairy-&gt;my dog is [mask]</li>
<li>10%的时候是随机的其他token。如：my dog is hairy-&gt;my dog is apple</li>
<li>10%的时候是原来的token。如：my dog is hairy-&gt;my dog is hairy</li>
</ul>
<p>再用该位置对应的 Ti 去预测出原来的token（输入到全连接，然后用softmax输出每个token的概率，最后用交叉熵计算loss）。<br />使用这种策略之后使得bert对所有的token都敏感。</p>
<p><a name="Wu0yU"></a></p>
<h3 id="4-Next-Sentence-Prediction（NSP）"><a href="#4-Next-Sentence-Prediction（NSP）" class="headerlink" title="4.Next Sentence Prediction（NSP）"></a>4.Next Sentence Prediction（NSP）</h3><p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://example.com">camille</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2023/03/05/Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8CTransformer%E5%92%8CBert/">http://example.com/2023/03/05/Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8CTransformer%E5%92%8CBert/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/03/05/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hello World</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">camille</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer"><span class="toc-number">1.</span> <span class="toc-text">transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E4%BF%97%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">通俗介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA"><span class="toc-number">3.</span> <span class="toc-text">理论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%8F%98%E4%BD%93"><span class="toc-number">4.</span> <span class="toc-text">注意力机制的变体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">自注意力模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">6.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoding%E9%83%A8%E5%88%86%EF%BC%9A"><span class="toc-number">6.1.</span> <span class="toc-text">Encoding部分：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder%E9%83%A8%E5%88%86%EF%BC%9A"><span class="toc-number">6.2.</span> <span class="toc-text">decoder部分：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bert"><span class="toc-number">7.</span> <span class="toc-text">Bert</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86"><span class="toc-number">7.1.</span> <span class="toc-text">1.输入部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86"><span class="toc-number">7.2.</span> <span class="toc-text">2.输出部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Masked-Language-Model-MLM-%E7%AC%AC%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="toc-number">7.3.</span> <span class="toc-text">3.Masked Language Model(MLM)(第一个预训练任务)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Next-Sentence-Prediction%EF%BC%88NSP%EF%BC%89"><span class="toc-number">7.4.</span> <span class="toc-text">4.Next Sentence Prediction（NSP）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/05/Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8CTransformer%E5%92%8CBert/" title="No title">No title</a><time datetime="2023-03-05T13:10:48.134Z" title="Created 2023-03-05 21:10:48">2023-03-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/05/hello-world/" title="Hello World">Hello World</a><time datetime="2023-03-05T12:18:04.883Z" title="Created 2023-03-05 20:18:04">2023-03-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By camille</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>